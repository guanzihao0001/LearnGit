{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Walk_through_lab_task2.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"nBGf6xh4Hg2K"},"source":["from IPython.display import display, Javascript\n","from google.colab.output import eval_js\n","from base64 import b64decode, b64encode\n","\n","import numpy as np\n","from PIL import Image\n","\n","import io\n","import cv2  as cv # OpenCV library"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ohdgQWXLjZVR","colab":{"base_uri":"https://localhost:8080/","height":37},"executionInfo":{"status":"ok","timestamp":1614434697711,"user_tz":-60,"elapsed":3496,"user":{"displayName":"Zihao Guan","photoUrl":"","userId":"07770799287110677933"}},"outputId":"f56ce67b-9b80-421d-ef9b-3f5dddaaa283"},"source":["cv.__version__"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'4.1.2'"]},"metadata":{"tags":[]},"execution_count":2}]},{"cell_type":"code","metadata":{"id":"zozRvvLQRFbC"},"source":["from google.colab import drive\n","drive.mount(\"/content/drive\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XS0UfPTMIAjH"},"source":["from google.colab.output import eval_js\n","\n","def VideoCapture():\n","  js = Javascript('''\n","    async function create(){\n","      div = document.createElement('div');\n","      document.body.appendChild(div);\n","\n","      video = document.createElement('video');\n","      video.setAttribute('playsinline', '');\n","\n","      div.appendChild(video);\n","      stream = await navigator.mediaDevices.getUserMedia({video: {facingMode: \"environment\"}});\n","      video.srcObject = stream;\n","\n","      await video.play();\n","\n","      canvas =  document.createElement('canvas');\n","      canvas.width = video.videoWidth;\n","      canvas.height = video.videoHeight;\n","      canvas.getContext('2d').drawImage(video, 0, 0);\n","\n","      div_out = document.createElement('div');\n","      document.body.appendChild(div_out);\n","      img = document.createElement('img');\n","      div_out.appendChild(img);\n","    }\n","\n","    async function capture(){\n","        return await new Promise(function(resolve, reject){\n","            pendingResolve = resolve;\n","            canvas.getContext('2d').drawImage(video, 0, 0);\n","            result = canvas.toDataURL('image/jpeg', 0.20);\n","\n","            pendingResolve(result);\n","        })\n","    }\n","\n","    function showimg(imgb64){\n","        img.src = \"data:image/jpg;base64,\" + imgb64;\n","    }\n","\n","  ''')\n","  display(js)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xRSMs81vI1kW"},"source":["def b64_to_bytes(byte):\n","  jpeg = b64decode(byte.split(',')[1])\n","  im = Image.open(io.BytesIO(jpeg))\n","  return np.array(im)\n","\n","def bytes_to_b64(image):\n","  image = Image.fromarray(image)\n","  buffer = io.BytesIO()\n","  image.save(buffer, 'jpeg')\n","  buffer.seek(0)\n","  x = b64encode(buffer.read()).decode('utf-8')\n","  return x"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pvm1YoLZKNow"},"source":["# Load cascades using cv2.CascadeClassifier()\n","face_cascade = cv.CascadeClassifier(\"/content/drive/MyDrive/mam4_CV/haarcascade_frontalface_alt.xml\")\n","eye_cascade = cv.CascadeClassifier('/content/drive/MyDrive/mam4_CV/haarcascade_eye.xml')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_2cv-gsMoRCg"},"source":["def detect(img, cascade):\n","    rects = cascade.detectMultiScale(img, scaleFactor=1.3, minNeighbors=4, minSize=(30, 30),\n","                                     flags=cv.CASCADE_SCALE_IMAGE)\n","    if len(rects) == 0:\n","        return []\n","    rects[:,2:] += rects[:,:2]\n","    return rects"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pbb0ctnDfJbx"},"source":["def detect_faces(img, cascades):\n","  # trnasform to gray\n","  # use cascades to detect faces\n","  gray = cv.cvtColor(img, cv.COLOR_RGB2GRAY)  #Convert ordinary pictures into grayscale images for computer processing\n","  gray = cv.equalizeHist(gray) #Count the values of these pixels in the image to get a unified overall gray concept\n","  faces = detect(gray,cascades) #Detect the position of the face\n","  return faces"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_yVIWlHefU-m"},"source":["# This function computes the sub region in which we would want to detect faces\n","# Params:\n","# previous: The previous face bounding box (x, y, w, h)\n","# image heigh and img_width: Self explaining and used to insure boundary constraints\n","# margin: How mach farther from the previous bounding box you would like to search for faces in the current frame \n","def compute_optimized_search_region(previous, img_height, img_width, margin):\n","  # Don't forget to insure boundary constraints:\n","  roi=[previous[0]-margin,previous[1]-margin,previous[2]+margin,previous[3]+margin]\n","  if(roi[0]<0):roi[0]=0\n","  if(roi[1]<0):roi[1]=0\n","  if(roi[2]>img_width):roi[2]=img_width\n","  if(roi[3]>img_height):roi[3]=img_height\n","  (x_new, y_new, w_new, h_new)=roi\n"," \n","  return (x_new, y_new, w_new, h_new)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dq5xvSVTIk1l","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"error","timestamp":1613396223365,"user_tz":-60,"elapsed":56122,"user":{"displayName":"Zihao Guan","photoUrl":"","userId":"07770799287110677933"}},"outputId":"a8fabc20-7636-417e-9434-03eaf792b3da"},"source":["# Video capture will enable your camera and start streaming\n","VideoCapture()\n","eval_js('create()')\n","\n","byte = eval_js('capture()')\n","im = b64_to_bytes(byte)\n","(img_height, img_width) = im.shape[0], im.shape[1] # To be used in the boundary constraints\n","\n","previous_bbox = None\n","margin = 40 # You can change this value as you want\n","\n","\n","while True:\n","  b64_im = eval_js('capture()') # Take a capture \n","  im = b64_to_bytes(b64_im) # Convert the capture brom base64 to bytes array\n","  region_to_use = im # \n","  \n","  if previous_bbox is not None: # If we have detected a face in the previous frame\n","    # Use the previous bounding box to compute the sub region in which you would like to seach for faces\n","    region_n = compute_optimized_search_region(previous_bbox, 480, 640, margin)# The new sub region\n","    region_to_use=im[region_n[1]:region_n[3],region_n[0]:region_n[2]]\n","    # Draw a red rectangle around the sub region in the 'im' image (because it's the one we will plot finally)\n","    cv.rectangle(im, (region_n[0], region_n[1]), (region_n[2], region_n[3]), (255,0,0), 3)\n","  faces = detect_faces(region_to_use, face_cascade) # detect faces in the sub region to use\n","\n","  if len(faces) == 1: # Assume one face is detected\n","    face = faces[0]\n","  \n","    # Update the face bounding box. Be careful: The coordinates should be mapped relative to the full image 'im' and not relative to the 'region_to_use'\n","    if region_to_use.all == im.all:\n","      previous_bbox=face\n","    else:\n","      previous_bbox=(region_n[0],region_n[1],region_n[0],region_n[1])+face\n","    # Draw a green rectangle around the detected face in the original image 'im'\n","    cv.rectangle(im, (previous_bbox[0], previous_bbox[1]), (previous_bbox[2], previous_bbox[3]), (0,255,0), 3) \n","  else: \n","    previous_bbox = None # If we did not detect any face or more than one face we just repeat the search using the entire image in the next frame\n","\n","  eval_js('showimg(\"{}\")'.format(bytes_to_b64(im))) # We convert our image with bounding boxes to base64 and plot it using JS "],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"application/javascript":["\n","    async function create(){\n","      div = document.createElement('div');\n","      document.body.appendChild(div);\n","\n","      video = document.createElement('video');\n","      video.setAttribute('playsinline', '');\n","\n","      div.appendChild(video);\n","      stream = await navigator.mediaDevices.getUserMedia({video: {facingMode: \"environment\"}});\n","      video.srcObject = stream;\n","\n","      await video.play();\n","\n","      canvas =  document.createElement('canvas');\n","      canvas.width = video.videoWidth;\n","      canvas.height = video.videoHeight;\n","      canvas.getContext('2d').drawImage(video, 0, 0);\n","\n","      div_out = document.createElement('div');\n","      document.body.appendChild(div_out);\n","      img = document.createElement('img');\n","      div_out.appendChild(img);\n","    }\n","\n","    async function capture(){\n","        return await new Promise(function(resolve, reject){\n","            pendingResolve = resolve;\n","            canvas.getContext('2d').drawImage(video, 0, 0);\n","            result = canvas.toDataURL('image/jpeg', 0.20);\n","\n","            pendingResolve(result);\n","        })\n","    }\n","\n","    function showimg(imgb64){\n","        img.src = \"data:image/jpg;base64,\" + imgb64;\n","    }\n","\n","  "],"text/plain":["<IPython.core.display.Javascript object>"]},"metadata":{"tags":[]}},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-10-1a51c560f024>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m   \u001b[0mb64_im\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval_js\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'capture()'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Take a capture\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m   \u001b[0mim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mb64_to_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb64_im\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Convert the capture brom base64 to bytes array\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m   \u001b[0mregion_to_use\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mim\u001b[0m \u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/output/_js.py\u001b[0m in \u001b[0;36meval_js\u001b[0;34m(script, ignore_result, timeout_sec)\u001b[0m\n\u001b[1;32m     38\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mignore_result\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0m_message\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_read_next_input_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_NOT_READY\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m       \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.025\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m       \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m     if (reply.get('type') == 'colab_reply' and\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]}]}